---
title: "ADS 599 Capstone: Modeling"
author: "Madeline Chang"
date: "2025-11-01"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(janitor)
library(lubridate)
library(caret)
library(rlang)
library(gridExtra)
library(corrplot)
library(forcats)
library(stats)
library(mosaic)
library(knitr)
library(weights)
library(Hmisc)
library(corrplot)
library(kernlab)
library(here)
library(pROC)
library(randomForest)

library(tidycensus)
```

```{r setup, include=FALSE}
model_data <- readr::read_csv('/Users/mtc/ADS/ADS 599/599 Project/model_data.csv')

model_data<- model_data %>%
  mutate(pov_ind = as.factor(pov_ind)) %>%
  select(-DRAT)

dummy <- dummyVars(~ COW_desc, data = model_data, levelsOnly = TRUE)
one_hot <- data.frame(predict(dummy, newdata = model_data))

col = colnames(one_hot)

col_n = str_remove(col, 'COW_desc')
names(one_hot) <- col_n

model_data<- model_data[,1:4] %>%
  cbind(one_hot) %>%
  cbind(model_data[,6:29])
```


# Anything from here down is not fully tested. The code is originally from a previous project and still needs to be adjusted to fit this project's data.

## Data Splitting
```{r}
set.seed(720)

# Create stratified random splits of the data (based on the classes)
trainingRows <- createDataPartition(model_data$pov_ind, p = .80, list = FALSE) 

# Subset data into train and test
train <- model_data[trainingRows, ]
test <- model_data[-trainingRows, ]


# Create control method for cross validation
ctrl <- trainControl(method = "cv",
                     number = 3,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)
```

**Model Building Strategies**
```{r warning=FALSE, message=FALSE}
# Function for Training Models
class_function<- function(method){
  model<- train(x = train[,3:37], 
               y = train$pov_ind,
               weights = train$PWGTP,
               method = method,
               preProc = c("center", "scale"),
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models with TuneGrids
class_grid<- function(method, grid){
  model<- train(x = train[,3:37], 
               y = train$pov_ind,
               weights = train$PWGTP,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               metric = "ROC",
               trControl = ctrl)
}


# Function for Training Models for Neural Networks
class_nnet<- function(method, grid, maxit){
  model<- train(x = train[,3:37], 
               y = train$pov_ind,
               weights = train$PWGTP,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               maxit = maxit,
               trace = FALSE, 
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models with Bagging
class_bag<- function(method, nbagg){
  model<- train(x = train[,3:37], 
               y = train$pov_ind,
               weights = train$PWGTP,
               method = method,
               preProc = c("center", "scale"),
               nbagg = nbagg,
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models with Random Forest
class_rf<- function(method, grid, ntree){
  model<- train(x = train[,3:37], 
               y = train$pov_ind,
               weights = train$PWGTP,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               ntree = ntree,
               metric = "ROC",
               trControl = ctrl)
}

```

**Hyper Parameter Tuning Defined in Grids**
```{r warning=FALSE, message=FALSE}
set.seed(720)

# TuneGrid for Penalized Logistic Regression
glmnGrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1),
                        lambda = seq(.01, .2, length = 10))

# NSC TuneGrid
nsc_grid<- data.frame(threshold = seq(0, 25, length = 30))


# NNET TuneGrid #########################################################
nnet_grid <- expand.grid(size=1:4, decay=c(0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4))

lassoGrid <- expand.grid(alpha = c(1),
                        lambda = seq(.1, .8, length = 10))

svmGrid<- data.frame(C = c(0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000))

# RF TuneGrid
mtryValues <- data.frame(mtry = seq(1,10,1))

```


```{r warning=FALSE, message=FALSE}
set.seed(720)

#| cache: true

# Generalized Linear Model: Logistic Regression
lr<- class_function("glm")

# Linear Discriminant Analysis
lda<- class_function("lda")

# Lasso LR
lasso<- class_grid("glmnet", lassoGrid)
plot(lasso, main = "Hyperparameter Tuning: Lasso")

# Penalized LR
glmn<- class_grid("glmnet", glmnGrid)
plot(glmn, main = "Hyperparameter Tuning: Penalized Logistic")

# Nearest Shrunken Centroid
nsc<- class_grid("pam", nsc_grid)
plot(nsc, main = "Hyperparameter Tuning: Nearest Shrunken Centroid")

# NNET
nnet<- class_nnet("nnet", nnet_grid, 20)
plot(nnet, main = "Hyperparameter Tuning: Neural Network")

# Bagging
bag<- class_bag("treebag", 50)

# Random Forest
rf<- class_rf("rf", mtryValues, 100)
plot(rf, main = "Hyperparameter Tuning: Random Forest")

```

**Model Performance**


```{r, warning=FALSE, message=FALSE}
train_re<- resamples(list(
  lr = lr,
  lda = lda,
  glmn = glmn,
  nsc = nsc,
  lasso = lasso,
  bag = bag,
  rf = rf,
  nnet = nnet
))

dotplot(train_re, metric = "ROC")
```



```{r, warning=FALSE, message=FALSE}
model_roc <- function(model){
  roc(response = model$pred$obs,
             predictor = model$pred$pov,
             levels = rev(levels(model$pred$obs)))
}

lrROC <- model_roc(lr)
ldaROC <- model_roc(lda)
glmnROC <- model_roc(glmn)
nscROC <- model_roc(nsc)
lassoROC<- model_roc(lasso)
bagROC <- model_roc(bag)
rfROC <- model_roc(rf)
nnetROC <- model_roc(nnet)


plot(lrROC, type = "s", col = 'red', legacy.axes = TRUE, 
     main = "Poverty Indicator Predictions ROC curves")
plot(ldaROC, type = "s", add = TRUE, col = 'green', legacy.axes = TRUE)
plot(glmnROC, type = "s", add = TRUE, col = 'blue', legacy.axes = TRUE)
plot(nscROC, type = "s", add = TRUE, col = 'pink', legacy.axes = TRUE)
plot(bagROC, type = "s", add = TRUE, col = 'purple', legacy.axes = TRUE)
plot(rfROC, type = "s", add = TRUE, col = 'cyan', legacy.axes = TRUE)
plot(nnetROC , type = "s", add = TRUE, col = 'yellow', legacy.axes = TRUE)
plot(lassoROC , type = "s", add = TRUE, col = 'grey', legacy.axes = TRUE)
legend("bottomright", legend=c(paste("LR (AUC =", round(lrROC$auc,4), ")"), 
                               paste("LDA (AUC =", round(ldaROC$auc,4), ")"), 
                               paste("GLMNET (AUC =", round(glmnROC$auc,4), ")"), 
                               paste("NSC (AUC =", round(nscROC$auc,4), ")"), 
                               paste("BAG (AUC =", round(bagROC$auc,4), ")"), 
                               paste("RF (AUC =", round(rfROC$auc,4), ")"), 
                               paste("NNET (AUC =", round(nnetROC$auc,4), ")"),
                               paste("LASSO (AUC =", round(lassoROC$auc,4), ")")),
       col=c("red", "green", "blue", "pink", "purple","cyan", "grey"), lwd=2)

```


```{r}
lr_Imp <- varImp(lr, scale = FALSE)
glmn_Imp <- varImp(glmn, scale = FALSE)
lasso_Imp<- varImp(lasso, scale = FALSE)
nsc_Imp <- varImp(nsc, scale = FALSE)

plot(lr_Imp, top = 10, main = "Top 10 Variables in Logistic Regression")
plot(glmn_Imp, top = 10, main = "Top 10 Variables in Penalized Logistic Regression")
plot(lasso_Imp, top = 10, main = "Top 10 Variables in Lasso Regression")
plot(nsc_Imp, top = 10, main = "Top 10 Variables in Nearest Shrunken Centroids")
```


```{r}
set.seed(720)
#| cache: true

ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

# Baseline LR and Selected Models
lr_r<- class_function("glm") 
glmn_r<- class_grid("glmnet", glmnGrid)
lda_r<- class_function("lda")
```

```{r}
train_re<- resamples(list(
  lr = lr_r,
  glmn = glmn_r,
  lda = lda_r
))

dotplot(train_re, metric = "ROC")
```


```{r}
lr_r_Imp <- varImp(lr_r, scale = FALSE)
glmn_r_Imp <- varImp(glmn_r, scale = FALSE)
lda_r_Imp <- varImp(lda_r, scale = FALSE)

plot(lr_r_Imp, top = 10, main = "Top 10 Variables for Repeated CV Logistic Regression")
plot(glmn_r_Imp, top = 10, main = "Top 10 Variables for Repeated CV Penalized Logistic")
plot(lda_r_Imp, top = 10, main = "Top 10 Variables for Repeated CV Linear Discriminant Analysis")

```


```{r}
testResults <- data.frame(
  obs = test$pov_ind,
  lr = predict(lr_r, test[, 3:37]),
  glmnet = predict(glmn_r, test[, 3:37]),
  lda = predict(lda_r, test[, 3:37])
)

selected_models <- c("lr", "glmnet", "lda")

for (model in selected_models) {
  cat("Confusion Matrix for", model, ":\n")
  print(confusionMatrix(testResults[[model]], testResults$obs, positive = "pov"))
  cat("\n")
}
```

LDA model has the best balanced accuracy.

## Regression

```{r}
near_to_pov<- model_data %>%
  select(!pov_ind)

trainingRows2<- createDataPartition(near_to_pov$POVPIP, p = .75, list = FALSE) 
train2<-  near_to_pov[trainingRows2, ]
test2 <- near_to_pov[-trainingRows2, ]
```




```{r}
# all variables
lm1<- lm(POVPIP ~ ., data = train2[,2:37], weights = train2$PWGTP)
summary(lm1)

# educational attainment with disability interactions
lm2<- lm(POVPIP ~ edu_cat + edu_cat*dis, data = train2, weights = PWGTP)
summary(lm2)

# educational attainment with disability interactions and standard hours worked per week
lm3<- lm(POVPIP ~ edu_cat + edu_cat*dis + WKHP, data = train2, weights = PWGTP)
summary(lm3)

# educational attainment with disability, standard hours worked per week, and geographical information
lm4<- lm(POVPIP ~ age + edu_cat + tract_in + edu_cat*dis + tract_in*dis, data = train2, weights = PWGTP)
summary(lm4)
```

```{r}
test2_Results <- data.frame(
  obs = test2$POVPIP,
  lm = predict(lm1, test2[, 3:37])
)

RMSE(test2_Results$obs, test2_Results$lm)
```

