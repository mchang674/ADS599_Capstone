---
title: "ADS 599 Capstone: Modeling"
author: "Madeline Chang"
date: "2025-11-01"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(janitor)
library(lubridate)
library(caret)
library(rlang)
library(gridExtra)
library(corrplot)
library(forcats)
library(stats)
library(mosaic)
library(knitr)
library(weights)
library(Hmisc)
library(corrplot)
library(kernlab)
library(here)
library(pROC)
library(randomForest)


library(tidycensus)
```

```{r setup, include=FALSE}
model_data <- readr::read_csv(here::here("model_data.csv"))

model_data<- model_data %>%
  mutate(pov_ind = as.factor(pov_ind)) %>%
  select(-DRAT)

dummy <- dummyVars(~ edu_cat +COW_desc, data = model_data, levelsOnly = TRUE)
one_hot <- data.frame(predict(dummy, newdata = model_data))

col = colnames(one_hot)

col_n = str_remove(col, 'edu_cat|COW_desc')
names(one_hot) <- col_n

model_data<- model_data[,1:3] %>%
  cbind(one_hot) %>%
  cbind(model_data[,6:29])
```

# Anything from here down is not fully tested. The code is originally from a previous project and still needs to be adjusted to fit this project's data.

## Data Splitting
```{r}
set.seed(720)

# Create stratified random splits of the data (based on the classes)
trainingRows <- createDataPartition(model_data$pov_ind, p = .80, list = FALSE) 

# Subset data into train and test
train <- model_data[trainingRows, ]
test <- model_data[-trainingRows, ]


# Create control method for cross validation
ctrl <- trainControl(method = "cv",
                     number = 3,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)
```
-MA Chunk
```{r}
#double check imbalance and representation in split
cat("Class Distribution Check:\n")

cat("Training Set:\n")
print(table(train$pov_ind))
print(prop.table(table(train$pov_ind)) * 100)

cat("\nTest Set:\n")
print(table(test$pov_ind))
print(prop.table(table(test$pov_ind)) * 100)
```


**Model Building Strategies**
```{r warning=FALSE, message=FALSE}
# Function for Training Models
class_function<- function(method){
  model<- train(x = train[,3:43], 
               y = train$pov_ind,
               weights = train$PWGTP,
               method = method,
               preProc = c("center", "scale"),
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models with TuneGrids
class_grid<- function(method, grid){
  model<- train(x = train[,3:43], 
               y = train$pov_ind,
               weights = train$PWGTP,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               metric = "ROC",
               trControl = ctrl)
}


# Function for Training Models for Neural Networks
class_nnet<- function(method, grid, maxit){
  model<- train(x = train[,3:43], 
               y = train$pov_ind,
               weights = train$PWGTP,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               maxit = maxit,
               trace = FALSE, 
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models with Bagging
class_bag<- function(method, nbagg){
  model<- train(x = train[,3:43], 
               y = train$pov_ind,
               weights = train$PWGTP,
               method = method,
               preProc = c("center", "scale"),
               nbagg = nbagg,
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models with Random Forest
class_rf<- function(method, grid, ntree){
  model<- train(x = train[,3:43], 
               y = train$pov_ind,
               weights = train$PWGTP,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               ntree = ntree,
               metric = "ROC",
               trControl = ctrl)
}

```

**Hyper Parameter Tuning Defined in Grids**
```{r warning=FALSE, message=FALSE}
set.seed(720)

# TuneGrid for Penalized Logistic Regression
glmnGrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1),
                        lambda = seq(.01, .2, length = 10))

# NSC TuneGrid
nsc_grid<- data.frame(threshold = seq(0, 25, length = 30))


# NNET TuneGrid #########################################################
nnet_grid <- expand.grid(size=1:4, decay=c(0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4))

lassoGrid <- expand.grid(alpha = c(1),
                        lambda = seq(.1, .8, length = 10))

svmGrid<- data.frame(C = c(0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000))

# RF TuneGrid
mtryValues <- data.frame(mtry = seq(1,10,1))

```


```{r warning=FALSE, message=FALSE}
set.seed(720)

#| cache: true

# Generalized Linear Model: Logistic Regression
lr<- class_function("glm")

# Linear Discriminant Analysis
lda<- class_function("lda")

# Lasso LR
lasso<- class_grid("glmnet", lassoGrid)
plot(lasso, main = "Hyperparameter Tuning: Lasso")

# Penalized LR
glmn<- class_grid("glmnet", glmnGrid)
plot(glmn, main = "Hyperparameter Tuning: Penalized Logistic")

# Nearest Shrunken Centroid
nsc<- class_grid("pam", nsc_grid)
plot(nsc, main = "Hyperparameter Tuning: Nearest Shrunken Centroid")

# NNET
nnet<- class_nnet("nnet", nnet_grid, 20)
plot(nnet, main = "Hyperparameter Tuning: Neural Network")

# Bagging
bag<- class_bag("treebag", 50)

# Random Forest
rf<- class_rf("rf", mtryValues, 100)
plot(rf, main = "Hyperparameter Tuning: Random Forest")

```

**Model Performance**


```{r, warning=FALSE, message=FALSE}
train_re<- resamples(list(
  lr = lr,
  lda = lda,
  glmn = glmn,
  nsc = nsc,
  lasso = lasso,
  bag = bag,
  rf = rf,
  nnet = nnet
))

dotplot(train_re, metric = "ROC")
```



```{r, warning=FALSE, message=FALSE}
model_roc <- function(model){
  roc(response = model$pred$obs,
             predictor = model$pred$pov,
             levels = rev(levels(model$pred$obs)))
}

lrROC <- model_roc(lr)
ldaROC <- model_roc(lda)
glmnROC <- model_roc(glmn)
nscROC <- model_roc(nsc)
lassoROC<- model_roc(lasso)
bagROC <- model_roc(bag)
rfROC <- model_roc(rf)
nnetROC <- model_roc(nnet)


plot(lrROC, type = "s", col = 'red', legacy.axes = TRUE, 
     main = "Poverty Indicator Predictions ROC curves")
plot(ldaROC, type = "s", add = TRUE, col = 'green', legacy.axes = TRUE)
plot(glmnROC, type = "s", add = TRUE, col = 'blue', legacy.axes = TRUE)
plot(nscROC, type = "s", add = TRUE, col = 'pink', legacy.axes = TRUE)
plot(bagROC, type = "s", add = TRUE, col = 'purple', legacy.axes = TRUE)
plot(rfROC, type = "s", add = TRUE, col = 'cyan', legacy.axes = TRUE)
plot(nnetROC , type = "s", add = TRUE, col = 'yellow', legacy.axes = TRUE)
plot(lassoROC , type = "s", add = TRUE, col = 'grey', legacy.axes = TRUE)
legend("bottomright", legend=c(paste("LR (AUC =", round(lrROC$auc,4), ")"), 
                               paste("LDA (AUC =", round(ldaROC$auc,4), ")"), 
                               paste("GLMNET (AUC =", round(glmnROC$auc,4), ")"), 
                               paste("NSC (AUC =", round(nscROC$auc,4), ")"), 
                               paste("BAG (AUC =", round(bagROC$auc,4), ")"), 
                               paste("RF (AUC =", round(rfROC$auc,4), ")"), 
                               paste("NNET (AUC =", round(nnetROC$auc,4), ")"),
                               paste("LASSO (AUC =", round(lassoROC$auc,4), ")")),
       col=c("red", "green", "blue", "pink", "purple","cyan", "grey"), lwd=2)

```


```{r}
lr_Imp <- varImp(lr, scale = FALSE)
glmn_Imp <- varImp(glmn, scale = FALSE)
lasso_Imp<- varImp(lasso, scale = FALSE)
nsc_Imp <- varImp(nsc, scale = FALSE)

plot(lr_Imp, top = 10, main = "Top 10 Variables in Logistic Regression")
plot(glmn_Imp, top = 10, main = "Top 10 Variables in Penalized Logistic Regression")
plot(lasso_Imp, top = 10, main = "Top 10 Variables in Lasso Regression")
plot(nsc_Imp, top = 10, main = "Top 10 Variables in Nearest Shrunken Centroids")
```


```{r}
set.seed(720)
#| cache: true

ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

# Baseline LR and Selected Models
lr_r<- class_function("glm") 
glmn_r<- class_grid("glmnet", glmnGrid)
lda_r<- class_function("lda")
```

```{r}
train_re<- resamples(list(
  lr = lr_r,
  glmn = glmn_r,
  lda = lda_r
))

dotplot(train_re, metric = "ROC")
```


```{r}
lr_r_Imp <- varImp(lr_r, scale = FALSE)
glmn_r_Imp <- varImp(glmn_r, scale = FALSE)
lda_r_Imp <- varImp(lda_r, scale = FALSE)

plot(lr_r_Imp, top = 10, main = "Top 10 Variables for Repeated CV Logistic Regression")
plot(glmn_r_Imp, top = 10, main = "Top 10 Variables for Repeated CV Penalized Logistic")
plot(lda_r_Imp, top = 10, main = "Top 10 Variables for Repeated CV Linear Discriminant Analysis")

```


```{r}
testResults <- data.frame(
  obs = test$pov_ind,
  lr = predict(lr_r, test[, 3:43]),
  glmnet = predict(glmn_r, test[, 3:43]),
  lda = predict(lda_r, test[, 3:43])
)

selected_models <- c("lr", "glmnet", "lda")

for (model in selected_models) {
  cat("Confusion Matrix for", model, ":\n")
  print(confusionMatrix(testResults[[model]], testResults$obs, positive = "pov"))
  cat("\n")
}
```
-MA chunk - displays for test results
```{r}
#pull performance from confusion matrix
lr_cm <- confusionMatrix(testResults$lr, testResults$obs, positive = "pov")
glmnet_cm <- confusionMatrix(testResults$glmnet, testResults$obs, positive = "pov")
lda_cm <- confusionMatrix(testResults$lda, testResults$obs, positive = "pov")
```
-MA chunk - summary table
```{r}
classification_summary <- data.frame(
  Model = c("Logistic Regression", "Penalized Logistic (GLMNET)", "LDA"),
  Accuracy = c(lr_cm$overall['Accuracy'],
               glmnet_cm$overall['Accuracy'],
               lda_cm$overall['Accuracy']),
  Sensitivity = c(lr_cm$byClass['Sensitivity'],
                  glmnet_cm$byClass['Sensitivity'],
                  lda_cm$byClass['Sensitivity']),
  Specificity = c(lr_cm$byClass['Specificity'],
                  glmnet_cm$byClass['Specificity'],
                  lda_cm$byClass['Specificity']),
  Precision = c(lr_cm$byClass['Precision'],
                glmnet_cm$byClass['Precision'],
                lda_cm$byClass['Precision']),
  F1 = c(lr_cm$byClass['F1'],
         glmnet_cm$byClass['F1'],
         lda_cm$byClass['F1']),
  Balanced_Accuracy = c(lr_cm$byClass['Balanced Accuracy'],
                        glmnet_cm$byClass['Balanced Accuracy'],
                        lda_cm$byClass['Balanced Accuracy'])
)

#display the table
kable(classification_summary, 
      digits = 4,
      caption = "Classification Model Performance on Test Set",
      row.names = FALSE)
```
-MA chunk - Best models by metric for test data
```{r}
# Best by each metric
best_acc <- which.max(classification_summary$Accuracy)
best_sens <- which.max(classification_summary$Sensitivity)
best_spec <- which.max(classification_summary$Specificity)
best_bal <- which.max(classification_summary$Balanced_Accuracy)

cat("Best Accuracy:\n")
cat("  ", classification_summary$Model[best_acc], "\n")
cat("  ", round(classification_summary$Accuracy[best_acc], 4), "\n\n")

cat("Best Sensitivity (Recall):\n")
cat("  ", classification_summary$Model[best_sens], "\n")
cat("  ", round(classification_summary$Sensitivity[best_sens], 4), "\n\n")

cat("Best Specificity:\n")
cat("  ", classification_summary$Model[best_spec], "\n")
cat("  ", round(classification_summary$Specificity[best_spec], 4), "\n\n")

cat("Best Balanced Accuracy:\n")
cat("  ", classification_summary$Model[best_bal], "\n")
cat("  ", round(classification_summary$Balanced_Accuracy[best_bal], 4), "\n")
```

```{r}
#predicted probabilities from the final retrained models
test_lr_probs     <- predict(lr_r,    test[, 3:43], type = "prob")[, "pov"]
test_glmnet_probs <- predict(glmn_r,  test[, 3:43], type = "prob")[, "pov"]
test_lda_probs    <- predict(lda_r,   test[, 3:43], type = "prob")[, "pov"]

#build ROC objects
lr_test_roc     <- roc(test$pov_ind, test_lr_probs,     levels = rev(levels(test$pov_ind)))
glmnet_test_roc <- roc(test$pov_ind, test_glmnet_probs, levels = rev(levels(test$pov_ind)))
lda_test_roc    <- roc(test$pov_ind, test_lda_probs,    levels = rev(levels(test$pov_ind)))

#ROC curves
plot(lr_test_roc, col="red", type="s", lwd=2,
     main="TEST SET ROC Curves — Poverty Classification")
plot(glmnet_test_roc, col="blue", add=TRUE, type="s", lwd=2)
plot(lda_test_roc, col="green", add=TRUE, type="s", lwd=2)

legend("bottomright",
       legend=c(
         paste("LR (AUC =", round(lr_test_roc$auc,4), ")"),
         paste("GLMNET (AUC =", round(glmnet_test_roc$auc,4), ")"),
         paste("LDA (AUC =", round(lda_test_roc$auc,4), ")")
       ),
       col=c("red","blue","green"),
       lwd=2)

```


-MA chunk - classification bar chart
```{r}
#shape for plotting
classification_long <- classification_summary %>%
  select(Model, Accuracy, Sensitivity, Specificity) %>%
  pivot_longer(cols = c(Accuracy, Sensitivity, Specificity),
               names_to = "Metric",
               values_to = "Value")

#grouped bar chart
ggplot(classification_long, aes(x = Model, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  geom_text(aes(label = round(Value, 3)), 
            position = position_dodge(width = 0.7),
            vjust = -0.5, size = 4, fontface = "bold") +
  scale_fill_manual(values = c("Accuracy" = "steelblue", 
                                "Sensitivity" = "red", 
                                "Specificity" = "darkgreen")) +
  labs(
    title = "Classification Model Performance Comparison (Test Set)",
    subtitle = "Comparing Accuracy, Sensitivity, and Specificity",
    y = "Score",
    x = "",
    fill = "Metric"
  ) +
  ylim(0, 1.1) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 13),
    plot.subtitle = element_text(hjust = 0.5, size = 11),
    legend.position = "bottom",
    legend.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 15, hjust = 1, face = "bold", size = 11),
    axis.text.y = element_text(size = 11)
  )
```



-MA chunk - calibration for evaluating probability accuracy
```{r}
#evaluate probability accuracy
#predicted vs. observed
testProbs <- data.frame(
  obs    = test$pov_ind,
  lr     = predict(lr_r,    test[, 3:43], type = "prob")[, "pov"],
  glmnet = predict(glmn_r,  test[, 3:43], type = "prob")[, "pov"],
  lda    = predict(lda_r,   test[, 3:43], type = "prob")[, "pov"]
)

#calibration function to bin predictions and compare to actuals
calib <- caret::calibration(
  obs ~ lr + glmnet + lda,
  data  = testProbs,
  class = "pov",     
  cuts  = 10         
)

xyplot(
  calib,
  auto.key = list(columns = 3),
  xlab = "Predicted probability of poverty",
  ylab = "Observed poverty rate",
  main = "Calibration of Selected Models on Test Set"
)
```


LDA model has the best balanced accuracy.

## Regression

```{r}
near_to_pov<- model_data %>%
  select(!pov_ind)

trainingRows2<- createDataPartition(near_to_pov$POVPIP, p = .75, list = FALSE) 
train2<-  near_to_pov[trainingRows2, ]
test2 <- near_to_pov[-trainingRows2, ]
```




```{r}
# all variables
lm1<- lm(POVPIP ~ ., data = train2[,2:43], weights = train2$PWGTP)
summary(lm1)

# educational attainment with disability interactions
lm2<- lm(POVPIP ~ assoc_bach + hs_dip_equiv + masters_pl + middle + some_coll + some_hs + assoc_bach*dis + hs_dip_equiv*dis + masters_pl*dis + some_coll*dis + some_hs*dis, data = train2, weights = PWGTP)
summary(lm2)

# educational attainment with disability interactions and standard hours worked per week
lm3<- lm(POVPIP ~ assoc_bach + hs_dip_equiv + masters_pl + middle + some_coll + some_hs + assoc_bach*dis + hs_dip_equiv*dis + masters_pl*dis + some_coll*dis + some_hs*dis + WKHP, data = train2, weights = PWGTP)
summary(lm3)

# educational attainment with disability, standard hours worked per week, and geographical information
lm4<- lm(POVPIP ~ age + assoc_bach + hs_dip_equiv + masters_pl + middle + some_coll + some_hs + tract_in + assoc_bach*dis + hs_dip_equiv*dis + masters_pl*dis + some_coll*dis + some_hs*dis + tract_in*dis, data = train2, weights = PWGTP)
summary(lm4)
```

```{r}
RMSE(lm1)
RMSE(lm2)
RMSE(lm1)
RMSE(lm1)
```
Here and Below: Contribution: Matt
Evaluate the regression model performance on test data
```{r}
 # Get predictions on test set
pred1 <- predict(lm1, newdata = test2)
pred2 <- predict(lm2, newdata = test2)
pred3 <- predict(lm3, newdata = test2)
pred4 <- predict(lm4, newdata = test2)
```

```{r}
# Calculate all metrics using postResample (from caret package)
cat("Calculating test set performance metrics...\n\n")

metrics1 <- postResample(pred1, test2$POVPIP)
metrics2 <- postResample(pred2, test2$POVPIP)
metrics3 <- postResample(pred3, test2$POVPIP)
metrics4 <- postResample(pred4, test2$POVPIP)

# Display individual model results
cat("LM1 (All Variables):\n")
print(metrics1)
cat("\n")

cat("LM2 (Education + Disability Interactions):\n")
print(metrics2)
cat("\n")

cat("LM3 (+ Work Hours):\n")
print(metrics3)
cat("\n")

cat("LM4 (+ Geography):\n")
print(metrics4)
cat("\n")
```

```{r}
# Create comprehensive comparison table
regression_comparison <- data.frame(
  Model = c("LM1: All Variables", 
            "LM2: Edu + Disability", 
            "LM3: + Work Hours", 
            "LM4: + Geography"),
  RMSE = c(metrics1['RMSE'], 
           metrics2['RMSE'], 
           metrics3['RMSE'], 
           metrics4['RMSE']),
  Rsquared = c(metrics1['Rsquared'], 
               metrics2['Rsquared'], 
               metrics3['Rsquared'], 
               metrics4['Rsquared']),
  MAE = c(metrics1['MAE'], 
          metrics2['MAE'], 
          metrics3['MAE'], 
          metrics4['MAE'])
)
```

```{r}
# Display comparison table
kable(regression_comparison, 
      digits = 3,
      caption = "Regression Model Performance Comparison on Test Set",
      row.names = FALSE)
```

Predicted vs Actual - not too helpful, hard to see. Leaving it for now. 
```{r}
pred_actual_data <- data.frame(
  Actual = rep(test2$POVPIP, 4),
  Predicted = c(pred1, pred2, pred3, pred4),
  Model = rep(c("LM1: All Variables", 
                "LM2: Edu + Disability", 
                "LM3: + Work Hours", 
                "LM4: + Geography"), 
              each = length(pred1))
)

ggplot(pred_actual_data, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.4, size = 1, color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, 
              color = "red", linetype = "dashed", size = 1) +
  geom_smooth(method = "lm", se = TRUE, 
              color = "darkgreen", size = 0.8, alpha = 0.2) +
  facet_wrap(~Model, ncol = 2) +
  labs(
    title = "Predicted vs Actual POVPIP by Model",
    subtitle = "Red dashed line = perfect predictions | Green line = actual model fit",
    x = "Actual Poverty Income Ratio",
    y = "Predicted Poverty Income Ratio",
    caption = "Points closer to red diagonal indicate better predictions"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
    plot.subtitle = element_text(hjust = 0.5, size = 10),
    plot.caption = element_text(hjust = 0.5, size = 9, color = "gray40"),
    strip.text = element_text(face = "bold", size = 11)
  )
```
Box Plot of Prediction Errors
```{r}
#calc errors based on actual minus prediction
errors <- data.frame(
  Error = c(test2$POVPIP - pred1,
            test2$POVPIP - pred2,
            test2$POVPIP - pred3,
            test2$POVPIP - pred4),
  Model = rep(c("LM1: All Variables", 
                "LM2: Edu + Disability", 
                "LM3: + Work Hours", 
                "LM4: + Geography"), 
              each = length(pred1))
)

#create box plot
ggplot(errors, aes(x = Model, y = Error, fill = Model)) +
  geom_boxplot(alpha = 0.7, outlier.alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", size = 1) +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Prediction Error Distribution by Model (Test Set)",
    subtitle = "Smaller box = better predictions | Red line = perfect predictions (zero error)",
    y = "Prediction Error (Actual - Predicted)",
    x = ""
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
    plot.subtitle = element_text(hjust = 0.5, size = 11),
    legend.position = "none",
    axis.text.x = element_text(angle = 15, hjust = 1, face = "bold")
  )
```
Bar Charge for RMSE comparison

```{r fig.width=10, fig.height=6}
ggplot(regression_comparison, aes(x = reorder(Model, RMSE), y = RMSE, fill = Model)) +
  geom_bar(stat = "identity", width = 0.6) +
  geom_text(aes(label = round(RMSE, 1)), 
            vjust = 2, fontface = "bold", size = 6, color = "white") +  # Changed!
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Model Performance Comparison: RMSE (Test Set)",
    subtitle = "Lower RMSE = Better Predictions",
    y = "Root Mean Squared Error (RMSE)",
    x = ""
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
    plot.subtitle = element_text(hjust = 0.5, size = 10),
    legend.position = "none",
    axis.text.x = element_text(angle = 15, hjust = 1, face = "bold", size = 11),
    axis.text.y = element_text(size = 11)
  )
```



```{r}
# Identify best models by each criterion
cat("\n\nModel Selection Summary:\n")
cat("------------------------\n")
cat("Best model by RMSE (lower is better): ", 
    regression_comparison$Model[which.min(regression_comparison$RMSE)], "\n")
cat("Best model by R² (higher is better):  ", 
    regression_comparison$Model[which.max(regression_comparison$Rsquared)], "\n")
cat("Best model by MAE (lower is better):  ", 
    regression_comparison$Model[which.min(regression_comparison$MAE)], "\n")

# Highlight best overall model
best_idx <- which.min(regression_comparison$RMSE)
cat("\nRecommended Model:", regression_comparison$Model[best_idx], "\n")
cat("  RMSE:      ", round(regression_comparison$RMSE[best_idx], 3), "\n")
cat("  R-squared: ", round(regression_comparison$Rsquared[best_idx], 3), "\n")
cat("  MAE:       ", round(regression_comparison$MAE[best_idx], 3), "\n")
```

